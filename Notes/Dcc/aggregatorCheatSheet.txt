

architecture
------------
Method
    Stages (serial)
        Outputs/Job (parallel)
            Steps (can be parallel, usually serial)

create method 
-------------
- sbt new broadinstitute/dig-aggregator-method.g8

build aws/core
--------------
- sbt publishLocal

emr tips
--------
- copy bootstrap lookup files to 
  - WORK_DIR="/mnt/var/${JOB_METHOD}"



worker setup
------------
- make dir and copy to /mnt/var/$JOB_METHOD/
  - aws s3 cp ..... <dest_dir>

- for resouyrces specified in stage class; they will be copied on deploy to ${JOB_BUCKET}/resources/${JOB_METHOD} bucket
  - aws s3 cp ${JOB_BUCKET}/resources/${JOB_METHOD}/fullBassetScript.py .

  - override def additionalResources = Seq(
        "fullBassetScript.py"
    )


how to run
-----------
- to test reprocess (no run due to no --yes)
  - sbt> run -c ../config.json --reprocess --only basset --clusters 10
  
- to include/exclude outputs
  - sbt> run -c ../config..json --reprocess --stage BassetStage --only T2D* --exclude *BMI

- to run in production, setting DRYRUN to true (basset only one stage, so fine)
  - sbt> run -c ../config.json --test --yes

- examples
  - run -c ../config.json --yes --stage MagmaCombineNcbiGenePValuesStage --reprocess

- GIVES ERROR:
  - run -c ../config..json --reprocess --stage BurdenbinningStage


troubleshooting
---------------
- pyspark errors
  -> containers
    -> applications
      -> stdout



scratch:
--------

20200825 - cheatsheet
- sbt new broadinstitute/dig-aggregator-method.g8
- run --yes --only BioIndex/burden BioIndexProcessor

- aggregator tips
  - sbt> run --yes --only BioIndex/burden BioIndexProcessor

- setup
  - if have issues dig-aws
    - checkout dig-aws alongside aggregator
    - run 'sbt publishLocal' from that directory -> go to ivy
  - scala rebel console to play with
    - can start it from sbt, better to have your jars from the project in the classpath

    
