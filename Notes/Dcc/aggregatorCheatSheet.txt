

architecture
------------
Method
    Stages (serial)
        Outputs/Job (parallel)
            Steps (can be parallel, usually serial)

create method 
-------------
- sbt new broadinstitute/dig-aggregator-method.g8

build aws/core
--------------
- sbt publishLocal

emr tips
--------
- copy bootstrap lookup files to 
  - WORK_DIR="/mnt/var/${JOB_METHOD}"

copy part files into one:
-------------------------
# download a common script for use
aws s3 cp "${JOB_BUCKET}/resources/scripts/getmerge-strip-headers.sh" .
chmod +x getmerge-strip-headers.sh
# download associations for this phenotype locally
./getmerge-strip-headers.sh "${OUT_DIR}/variant-associations/${PHENOTYPE}/part-*" ./associations.csv

worker setup
------------
- make dir and copy to /mnt/var/$JOB_METHOD/
  - aws s3 cp ..... <dest_dir>

- for resouyrces specified in stage class; they will be copied on deploy to ${JOB_BUCKET}/resources/${JOB_METHOD} bucket
  - aws s3 cp ${JOB_BUCKET}/resources/${JOB_METHOD}/fullBassetScript.py .

  - override def additionalResources = Seq(
        "fullBassetScript.py"
    )


how to run
-----------
- to test reprocess (no run due to no --yes)
  - sbt> run -c ../config.json --reprocess --only basset --clusters 10
  
- to include/exclude outputs
  - sbt> run -c ../config..json --reprocess --stage BassetStage --only T2D* --exclude *BMI

- to run in production, setting DRYRUN to true (basset only one stage, so fine)
  - sbt> run -c ../config.json --test --yes

- to run only one stage
  - run -c ../config.json --yes --stage MagmaCombineNcbiGenePValuesStage --reprocess

- update that these phenotypes ran (update aggregator db)
  - ./run.sh finemapping --stage RunCojoStage --insert-runs --only `cat /home/javaprog/Data/Broad/dig-analysis-data/out/finemapping/cojo.txt` --yes
  
- GIVES ERROR:
  - run -c ../config..json --reprocess --stage BurdenbinningStage


troubleshooting
---------------
- pyspark errors
  -> containers
    -> applications
      -> stdout

monitoring:
-----------
- install htop
  - sudo yum install -y htop
- track steps that ran
  - aws s3 ls s3://dig-analysis-data/out/finemapping/cojo-results/ --recursive | awk -F '/' '{print $4}' | sort | uniq | xargs echo -n | sed -e "s/ /,/g" > cojo.txt
- run the steps and 

VM options:
-----------
  - Strategy.computeOptimized(vCPUs = 32) -> c5.9xlarge, 32 cpu, 72 gig
  - Strategy.memoryOptimized(mem = 128.gb) -> r5.4xlarge - 16 cpu, 128 gig
  - Ec2.Strategy.generalPurpose(mem = 64.gb) -> 


scratch:
--------

20200825 - cheatsheet
- sbt new broadinstitute/dig-aggregator-method.g8
- run --yes --only BioIndex/burden BioIndexProcessor

- aggregator tips
  - sbt> run --yes --only BioIndex/burden BioIndexProcessor

- setup
  - if have issues dig-aws
    - checkout dig-aws alongside aggregator
    - run 'sbt publishLocal' from that directory -> go to ivy
  - scala rebel console to play with
    - can start it from sbt, better to have your jars from the project in the classpath

    
