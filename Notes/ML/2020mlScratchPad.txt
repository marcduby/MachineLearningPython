

20221107 - mit ml seminar
- with single cell assays
  - can measure
    - rna Seq 
    - atac seq
    - chromatin accessibility
    - chromatin 3d 
    - gene expression 
  - want toi translation/predict what other modlity of measurement will be 
    - ie: from gene expression and chromatin accessibility, predict atac seq 
  - for learning 
    - need measurements from same cell for training data 
      - most assays are destructive, so only get one reading per cell 
      - use non destructive assay

- auto encoder
  - get representation of batch effects and sequencing depth 
- peptides 
  - sequence of amino acids (10-15 size?)
  - get observed spectrum from mass spectometer for each peptide 
  - the order of the amino acids affects the mass spectrometer reading curve 
  - only 1.4% overlap in peptides between species 
    - ? -> research 
- for transformers in NLP 
  - learning pairwise relationships between words 
  - can we use that for spectrometer peaks 

- ml models for genetics mentionned
  - babel 2021 



20221027 - broadway - ml in clinical applications
- questions to use ML 
  - what are protective mech for disease in the genome (like pcsk9)
  - how well infer rna expression from dna
  - can we combine omics data to predict outcomes and mech insights 
  - can we discover all endophenotypes (like BMI types)

- misc 
  - if unfold DNA, should be 6 feet long 
  - BMI - ratio of weight to surface area 


20201223 - pipeline work
- wrote test pipeline script using CA housing data
  - standard scalar didn't help with R2 score 
  - pca useless as well, best R2 score with full features
  - there is a way to modify pipelines with 
    - new_pipeline = Pipeline(old_pipeline.steps[1..2])
    - new_pipeline = Pipeline(old_pipeline.steps[1] + old_pipeline.stape[3])

- todo
  - try pca with correlated features
    - script correlation, then determine best dim for pca

    
20200426 - Hamburg PyData meetup - surrogate models
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, AlphaDropout

num_features = 2
num_categories = 3

dropout = 0.4
selu_dropout_model = tf.keras.Sequential()
selu_dropout_model.add(Input(name='input', shape=(num_features,)))
selu_dropout_model.add(Dense(name='hidden1', units=500, activation='selu'))
selu_dropout_model.add(AlphaDropout(dropout))
selu_dropout_model.add(Dense(name='hidden2', units=250, activation='selu', kernel_initializer=tf.initializers.lecun_normal(random_seed)))
selu_dropout_model.add(AlphaDropout(dropout))
selu_dropout_model.add(Dense(name='output', units=num_categories, activation='softmax'))

selu_dropout_model.compile(loss='sparse_categorical_crossentropy',
             optimizer=tf.keras.optimizers.Adam(),
             metrics=['accuracy'])
             
- use surrogate model for explainability
- steps
  - use NN to train on training data
  - fit the model
  - then create rgularized data and predict
  - use that result set to train a decision tree
  - now can use decision tree to explain model, test parameters
  
- decision trees overfot but are good for explainability
- NN models are generalized but black boxes

- alphadropout layer -> keeps mean and variance of inputs to their original values
  - best if used with selu activation on the debse layers
  
- selu -> scaled exponential linear units

