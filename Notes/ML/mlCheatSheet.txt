



NN layers:
----------


NN Layer Types:
---------------
- Convolution 2D:
  - filters that tease out features of the images that are then fed into the NN dense layers
  - ie: tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape=(28, 28, 1))
- Embedding for nlp
  - creates word vectors based on all the training data
    - the first dimension should match the number of different words in the train/test data
    - second parameter defines the dimensionality of the word vector space
  - ie: model.add(keras.layers.Embedding(10000, 16))
- Pooling
  - reduce dimensionality for smaller footprint
- MaxPooling2D
  - layer to reduce the given shape to the highest number in that matrix shape
  - keeps features, can sometimes accentuate them and reduces data passed on
  - used after convolution layer for image NN
  - ie: tf.keras.MaxPooling2D(2, 2)
- Dense
  - fully connected layer to the next layer
  - ie: tf.keras.layers.Dense(128, activation='relu')
 - Flatten
   - takes multi dimensionsl data and flattens to 1D; provide dimensionality params for the data
   - ie: tf.keras.layers.Flatten(input_shape=(28, 28))
- Dropout
  - layer randomly assigns outgoing edge of hidden layer to 0 for each training run, preventing ovefitting
- AlphaDropout

Activation:
-----------
- reLu - fast
- selu - 
- sigmoid - 0/1
- softmax - distribution of probabilities that add to 1

Hyper Parameters:
-----------------
- epochs - number times go through entire training set during training
- iterations - how many times to get through traing set based on batch size
- batch size: number of samples processed before model gradients are updated
  - to avodi loading all data into memory
  - compute gradient and take learning rate step towards gradient after one batch gone hrough training
- validation split - how to split off training data for validation
- shuffle - shuffle the validation/training mix after every epoch

PreProcessing:
--------------
- normalizing data - normalize nmbers so that they range from 0 to 1

Loss:
-----
- categorical_crossentropy - expects one hot matrix of what the labels are, not integer collection
- sparse_categorical_crossentropy - can use input labels of multiple integer values
- tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) - same as above?
- binary_crossentrop - use for y/n classification (sigmoind)

Optimizer:
----------
- adam - 

Metrics:
--------
 accuracy -
 
Misc:
-----
- cross validation
  - split train data into 5ths and rotate what segment is validation each round
    - average efficiency afterwards
  - pooling
    - used to reduce data for better compute performance
  
- transfer learning
  - repurpose model from similar problem, retrining last fully connected layer
    

Tips:
-----
- for first dense layer, take 20% size of previous layer


Pandas:
-------
- df.samples(20)
- df.value_counts().head()



NN Examples:
------------
- convolution neural network
  model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(64, (3, 3,), activation= 'relu', input_shape= (28, 28, 1),   # 64 filters of 3x3 size to go through the image
    tf.keras.layers.MaxPooling2D(2, 2),                                                 # 2x2 pooling, so take max number in 2x2 matrix
    tf.keras.layers.Conv2D(64, (3, 3,), activation= 'relu',
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation= 'relu'),
    tf.keras.layers.Dense(10, activation= 'softmax')
  ])