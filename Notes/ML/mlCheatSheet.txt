



NN layers:
----------


NN Layer Types:
---------------
- Convolution 2D:
  - filters that tease out features of the images that are then fed into the NN dense layers
  - ie: tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape=(28, 28, 1))
- Embedding for nlp
  - creates word vectors based on all the training data
    - the first dimension should match the number of different words in the train/test data
    - second parameter defines the dimensionality of the word vector space
  - ie: model.add(keras.layers.Embedding(10000, 16))
- Pooling
  - reduce dimensionality for smaller footprint
- MaxPooling2D
  - layer to reduce the given shape to the highest number in that matrix shape
  - keeps features, can sometimes accentuate them and reduces data passed on
  - used after convolution layer for image NN
  - ie: tf.keras.MaxPooling2D(2, 2)
- Dense
  - fully connected layer to the next layer
  - ie: tf.keras.layers.Dense(128, activation='relu')
 - Flatten
   - takes multi dimensionsl data and flattens to 1D; provide dimensionality params for the data
   - ie: tf.keras.layers.Flatten(input_shape=(28, 28))
- Dropout
  - layer randomly assigns outgoing edge of hidden layer to 0 for each training run, preventing overfitting
- AlphaDropout

Activation:
-----------
- reLu - fast
- selu - 
- sigmoid - 0/1
- softmax - distribution of probabilities that add to 1

Hyper Parameters:
-----------------
- epochs - number times go through entire training set during training
- iterations - how many times to get through traing set based on batch size
- batch size: number of samples processed before model gradients are updated
  - to avodi loading all data into memory
  - compute gradient and take learning rate step towards gradient after one batch gone hrough training
- validation split - how to split off training data for validation
- shuffle - shuffle the validation/training mix after every epoch

PreProcessing:
--------------
- normalizing data - normalize nmbers so that they range from 0 to 1

Loss:
-----
- categorical_crossentropy - expects one hot matrix of what the labels are, not integer collection
- sparse_categorical_crossentropy - can use input labels of multiple integer values
- tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) - same as above?
- binary_crossentrop - use for y/n classification (sigmoind)

Optimizer:
----------
- adam - 

Metrics:
--------
 accuracy -
 
Misc:
-----
- cross validation
  - split train data into 5ths and rotate what segment is validation each round
    - average efficiency afterwards
  - pooling
    - used to reduce data for better compute performance
  
- transfer learning
  - repurpose model from similar problem, retrining last fully connected layer
    

Tips:
-----
- for first dense layer, take 20% size of previous layer


Numpy:
------
- np.diagonal()   - diagonal rows as list
- np.sum()        - sum of all data
- np.trace()      - sum of diagonal

Pandas:
-------
- df.samples(20)
- low_df = result_df.query('score < 2.4e-07')         # filter
- low2_df = result_df.loc[result_df['score'] < 1.0e-07]         # filter
- df['column'].value_counts().head()
- df.sample(frac=1).reset_index(drop=True)                     # shuffle data
- df.loc[-len(df)] = [name, accuracy]                         # append data
- df = df.sort_values(by=['2'], ascending=False)            # sort
- df['income'].fillna((df['income'].mean()), inplace=True)   # replace NA with mean
- pd.concat([df1['c'], df2['c']], axis=1, keys=['df1', 'df2'])    # concatenate columns of 2 otjer dataframes

SkLearn:
--------
- dimensionality reduction
  - isomap for non linear data (lilke images)
  
Python:
-------
- pip freeze > requirements.txt             # save pip libs

Steps:
------
- load data
  - pd.read_csv()
- look for na data, remove or replace
  - df.dropna()
- split out the labels/features
  - labels = features.pop("Survived")
- one hot encode
  - pd.get_dummies()
- shuffle
  - sk.utils.shuffle()
- split into train/test
  - sklearn.model_selection.train_test_split
- scale the data (fit the scaler on train, then apply to train and test)
- build and compile model
- predict and measure accuracy (confusion matrix)


NN Examples:
------------
- convolution neural network (for images, first 2d layer extracts features)
  model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(64, (3, 3,), activation= 'relu', input_shape= (28, 28, 1),   # 64 filters of 3x3 size to go through the image
    tf.keras.layers.MaxPooling2D(2, 2),                                                 # 2x2 pooling, so take max number in 2x2 matrix
    tf.keras.layers.Conv2D(64, (3, 3,), activation= 'relu',
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation= 'relu'),
    tf.keras.layers.Dense(10, activation= 'softmax')
  ])
