

20201227 - ensemble learners
https://www.youtube.com/watch?v=WtWxOhhZWX0&t=540s
- learn several simple models and combne outputs to produce final decision
- 2 groups
  - sequential ensembl;e methods
    - label previous mislabeeld examples with higher weights
    - boosting
  - parrallel ensemble nmetjhods
    - ie: random forest
    - averaging results
      - can alseo due weighted avg (sum weghts = 1); give higher weight to model expected to perform better
- 2 metrics
  - robustness: takes in precitions of all the ensemble models
  - accuracy: improved performance due to lessening of bias/overfitting
- bagging
  - also called bootstrap aggregation
  - reduce variance by taking mean of mutiple estimates
  - process  
    - create randomly sampled datasets from original datasets
    - build and fit several classiiers to each
    - then take avg
- random forests
  - create random subsets of features
  - create decorrelated decision trees
- boosting 
  - the errors of the previous model are corrected by the subsequent model
    - for each step, give more weight to the incorrectly classified points from the previous step
    - then combine the N learners for final classification 
      - give higher weight to more accurate classifier
    - alpha factor of adaboost
      - alpha_t = (1/2) * ln((1 - Et)/Et)
      - where error of step t is (correct -N )/N
  - can use weak learners
  - ie: each new tree is a fity on a modified version of the original datasets

  - gradient boosting (GBM)
    - each new model minimizes the loss function usign the gradient descent method
      - new learner that is associated with the maximum minimizing of the loss function

  - xgboost (extreme gradient boosting)
    - library for developing fast and high performance gradient boosting tree models
      - good at not overfitting
      - tree based algorithm for classification, regression and ranking with custom loss functions
      - more regularized model to reduce overfitting over gradient boost approach
    - features
      - tree construction in parallel using all cores while training
      - can train in distributed cluster of machines for lareg models
      - handles sparse data (missing values)
      - can boost already fitted model on new data 
    - parameters
      - general
        - number of threads
          - if not entered, will auto detect number cores
        - booster
          - gbtree: tree based model (faster than linear)
          - gblinear: linear function
        - silent (0 will print msessages)
      - task 
        - objective
          - reg:linear
            - default rmse evaluation metric
          - binary:lofistic: log regression for binary classification
            - error for class metric 
          - multi:softmax: multi class (specify num_class)
            - mean avg precision for wevaluation metric 
        - evaluation metrics
      - bosster params 
        - step size
        - regularization
        - for gbtree 
          - eta for step size shrinkage
          - gamma
          - max_depth
        - for gblinear 
          - lambda: higher is more conservative











20201222 - sklearn pipelines for production - pydata 2016
- pipelines are containers of steps
  - transformer
    - 2 methods - transform and fit (rarely used)
  - estimator
    - fit() and predict() methods 
  - pipelines- featureUnion
- use pipleines to package workflow and fit model into single object
- FeatureUnion joins the results of two pipleine outputs
- why pipleines
  - transformations written out once
  - easy to swap out pieces
  - readability
  - keep all intermediate steps together
  - support GridSearch
  - can back up models in version control (code and pkl file)
  - peers can check out and test models 
- a pipeline is a set of transformers with an estimator at the bottom
  - for transform() call, calls fit() and transform() on each contained object
  - for predict() call, call transform() on each step then predict() on the last one
- FeatureUnion is a horizontal pipeline
  - feeds the results of the parrallel pipelines into a single merged output 

- steps in model development
  - feature engineering
  - model selection
  - hyperparameter optimization
  - cross validation
  - publish
- saving pipeline to disk 
  - joblib.dump(pipeline, './fit_pipeline.pkl')
  - saved_pipeline = joblib.load('./fit_pipeline.pkl')
- predict process 
  - load model
  - load input data 
  - model.predict(data)
  - write putput to db 



20201203 - neo4j model ml 
- notes
  - node embeddings of a graph
    - a way to reduce dimensionality of graph, avoiding sparse connection matrix
    - should be able to rebuild graph architecture
    - algorithms
      - fastRP
        - construct similarity matrix, then multiply with random projection
        - preserves distance of nodes
        - get dimensionality preserving lower dimensionality matrix
        - 75x faster than node2vec
        - add nodes, need to redo all embeddings
      - node2vec
        - for every node in graph, take random walk; train model to predict the connections
        - given values of hidden model layer
        - slow, so billion node graph is issue
        - add nodes, need to redo all embeddings
      - graphsage
        - predictove model, given node, predicts what embeddings could be 
        - samples nodes near any given node, using node properties
        - reduces loss function, builds model to be able to rebuild graph
        - uses GNN 
        - can generate new embeddings as nodes are added
  - what to use embeddings for
    - neighborhood predictions
    - classification
    - visualization

- https://github.com/AliciaFrame/GDS_Retail_Demo



20201202 - qcon - pytorch to production
- https://www.youtube.com/watch?v=EkELQw9tdWE
- https://qconnewyork.com/system/files/presentation-slides/jeff_smith_-_pytorch_qcon_ny_2019.pdf

- notes
  - modules
    - torch.data (datasets and dataoaders)
    - torch.vision -> data, models
    - torck.jit -> deploy graph to language neutral structure
    - torch.nn -> base
  - hyper parameter tuning
    - bayesopt
      - botorch.org
    - also look at
      - pytext
      - translate
      - horizon for RL
      - detectron
  - https://pytorch.org/ecosystem/
  

- saving/loading checkpoints
torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            ...
            }, PATH)

model = TheModelClass(*args, **kwargs)
optimizer = TheOptimizerClass(*args, **kwargs)

checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

model.eval()
# - or -
model.train()
