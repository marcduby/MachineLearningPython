
- environment
  - PYTHONPATH
  
- pip 
  - pip freeze > requirements.txt
  - pip install -r requirements.txt
  - pip install PackageName==1.4

new ubuntu install:
-------------------
- sudo apt update
- sudo apt install python3-pip
- sudo apt install python3.10-venv
- python3 -m venv VirtualEnv/pubmed


rest:
-----

json:
-----
  - with open('strings.json') as json_data:
      d = json.loads(json_data)
      json_data.close()
      pprint(d)

  - import json
    with open('strings.json') as f:
      d = json.load(f)
      print(d)

  - display json 
  print("build trapi payload: \n{}".format(json.dumps(payload, indent=2))
    
virtual env:
------------
  - python3 -m venv tutorial-env

strings:
--------
  a_string = "A string is more than its parts!"
  matches = ["more", "wholesome", "milk"]

  if any([x in a_string for x in matches]):

  str.lentgh()
  str[:300]

timer:
------


pandas:
-------
  - df_results = pd.DataFrame.from_dict(list_gene_chemical)
  
lists:
------
  - chunks = [data[x:x+100] for x in range(0, len(data), 100)]      # split list into sublists of size
  - chunks.append(element)
  - list_diseases = list(set(list_diseases))                        # get unique elements in the list
  - newlist = [word for word in words if len(word) == 9]
  - if isinstance(input_object, list)                               # one list enterily in another list
  - if any(ext in url_string for ext in extensionsToCheck):
        print(url_string)

    list_input = [STR_INPUT1, STR_INPUT2, STR_INPUT3]
    str_input = " ".join(list_input)    

  sub_size = 3
  for i in range(0, len(my_list), sub_size):
      sub = my_list[i:i+sub_size] 
      print(sub)

dict:
-----
  - dict.get('key')     # null safe way of getting element
  - if 'key' in dict:
  - dict.keys()
  - dict.values()
  - list(dict.keys())
  - list(dict.values())
  - del dict['key']
  - for key in dict:
    -- get key 

    
os:
---
  - to get list of regexp files
    import glob
    print(glob.glob("/home/adam/*.txt"))


  # Get the directory path
  dir_path = '/path/to/directory' 
  # List to store files
  files = [] 
  # Iterate over directory contents
  for entry in os.scandir(dir_path):
      # If entry is a file, store it
      if entry.is_file():
          files.append(entry.name) 


files:
------
  - reading multiple file with wildcard
  files = [file for file in glob.glob("../somefolder/*")]
  for file_name in files:
      with io.open(file_name, 'rb') as image_file:
          content = image_file.read()

  - read line by line
  with open(filename) as file:
    for line in file:
        print(line.rstrip())

jupyter:
--------
- expand seen rows for pandas 
  from IPython.display import display
  pd.options.display.max_rows = 999
  display(df_results)

exception:
----------
  x = 5
  y = "hello"
  try:
      z = x + y
  except TypeError:
      print("Error: cannot add an int and a str")
  

misc:
-----
  - if x is not None:

databases:
----------
  db = pymysql.connect(host='localhost', port=3306, user='root')
  cur = db.cursor()  
  cur.execute(sql).fetchall()/fetchone()


sqlalchemy:
-----------
- return new rowid
    with engine.connect() as conn:
        sql_params = data.dict()
        sql_params.update({'s3_bucket_id': s3_record_id, 'metadata': json.dumps(data.metadata)})
        res = conn.execute(text("""
            INSERT INTO records (s3_bucket_id, name, metadata, data_source_type, data_source, data_type, genome_build,
            ancestry, data_submitter, data_submitter_email, institution, sex, global_sample_size, t1d_sample_size, 
            bmi_adj_sample_size, status, additional_data) VALUES(:s3_bucket_id, :name, :metadata, :data_source_type, 
            :data_source, :data_type, :genome_build, :ancestry, :data_submitter, :data_submitter_email, :institution, 
            :sex, :global_sample_size, :t1d_sample_size, :bmi_adj_sample_size, :status, :additional_data)
        """), sql_params)
        conn.commit()
        s3.create_record_directory(s3_record_id)
    return s3_record_id, res.lastrowid

objects:
--------
  - __init__(self)
  - __repl__(self)
  - __str__(self)

  gene_association: GenePhenotypeAssociation
  for gene_association in list_gene_assoc:

debug:
------
  - help(<object>)
  - print(<object>)
  - type(cifar10).__mro__   # returns the class hierarchy
  - dir(package)            # get the listing of the functions in the package

to test:
--------
- formatting json
  - print(json.dumps(json.loads(file.split('\n')[0]), indent=2))
- python -m pytest -vv
  - adds curent directory to pytest 

notes:
------
I'm not familiar with green. Looks like it could be useful for running tests. I'm not sure if it'd run our most common scenario which is "pytest -d --tx '6*popen//python=python'"
Which runs all tests spread across 6 cores.



flask:
------
  - __main__.py has the port setting
  
helpful libs:
-------------
- PyPDF2 - read PDFs
- nltk - word vectorizing


scala code:
-----------
- cluster definitions
  /** Simple cluster with more memory. */
  override val cluster: ClusterDef = super.cluster.copy(
    masterInstanceType = Strategy.generalPurpose(mem = 64.gb),
    slaveInstanceType = Strategy.generalPurpose(mem = 32.gb),
    instances = 4             # 4 VMs for each cluster (one master, other slaves)
    applications = Seq.empty  # if only sh/python script, this will avoid having hadoop installed -> faster startup
    stepsConcurency = 3       # if only sh/python script, if > 1, then will run scripts in parallel on master node (where all non pyspark processes run)
  )

- copying dependent code to s3 resources directory
  /** Additional resources to upload. */
  override def additionalResources: Seq[String] = Seq(
    "runMETAL.sh",
    "getmerge-strip-headers.sh"
  )

- to merge csv files and strip headers except first one; this way can use pyspark to create files to merge in parallel 
aws s3 cp s3://.../getmerge-strip-headers.sh .
chmod +x getmerge-strip-headers.sh
./getmerge-strip-headers.sh s3://dig-analysis-data/out/magma/step1GatherVariants/part-* ./variants.csv


vs code:
--------
- interactive python
  - # %%

  