
- pip 
  - pip freeze > requirements.txt
  - pip install -r requirements.txt
  - pip install PackageName==1.4

json:
-----
  - with open('strings.json') as json_data:
      d = json.loads(json_data)
      json_data.close()
      pprint(d)

  - import json
    with open('strings.json') as f:
      d = json.load(f)
      print(d)
    
virtual env:
------------
  - python3 -m venv tutorial-env

lists:
------
  - chunks = [data[x:x+100] for x in range(0, len(data), 100)]      # split list into sublists of size
  - chunks.append(element)

dict:
-----
  - dict.get('key')     # null safe way of getting element
  - if 'key' in dict:
  - dict.keys()
  - dict.values()

os:
---
  - to get list of regexp files
    import glob
    print(glob.glob("/home/adam/*.txt"))

misc:
-----
  - if x is not None:

databases:
----------
  db = pymysql.connect(host='localhost', port=3306, user='root')
  cur = db.cursor()  
  cur.execute(sql).fetchall()/fetchone()

objects:
--------
  - __init__(self)
  - __repl__(self)
  - __str__(self)

debug:
------
  - help(<object>)
  - print(<object>)
  - type(cifar10).__mro__   # returns the class hierarchy
  - dir(package)            # get the listing of the functions in the package

to test:
--------
- formatting json
  - print(json.dumps(json.loads(file.split('\n')[0]), indent=2))

notes:
------
I'm not familiar with green. Looks like it could be useful for running tests. I'm not sure if it'd run our most common scenario which is "pytest -d --tx '6*popen//python=python'"
Which runs all tests spread across 6 cores.



scala code:
-----------
- cluster definitions
  /** Simple cluster with more memory. */
  override val cluster: ClusterDef = super.cluster.copy(
    masterInstanceType = Strategy.generalPurpose(mem = 64.gb),
    slaveInstanceType = Strategy.generalPurpose(mem = 32.gb),
    instances = 4             # 4 VMs for each cluster (one master, other slaves)
    applications = Seq.empty  # if only sh/python script, this will avoid having hadoop installed -> faster startup
    stepsConcurency = 3       # if only sh/python script, if > 1, then will run scripts in parallel on master node (where all non pyspark processes run)
  )

- copying dependent code to s3 resources directory
  /** Additional resources to upload. */
  override def additionalResources: Seq[String] = Seq(
    "runMETAL.sh",
    "getmerge-strip-headers.sh"
  )

- to merge csv files and strip headers except first one; this way can use pyspark to create files to merge in parallel 
aws s3 cp s3://.../getmerge-strip-headers.sh .
chmod +x getmerge-strip-headers.sh
./getmerge-strip-headers.sh s3://dig-analysis-data/out/magma/step1GatherVariants/part-* ./variants.csv

