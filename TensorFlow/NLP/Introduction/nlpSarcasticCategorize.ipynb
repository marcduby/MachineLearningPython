{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import urllib.request\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "got data of type: <class 'http.client.HTTPResponse'>\ngot json data of type: <class 'str'>\n"
    }
   ],
   "source": [
    "# open the training file\n",
    "url = \"https://raw.githubusercontent.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection/master/Sarcasm_Headlines_Dataset.json\"\n",
    "# url = \"https://raw.githubusercontent.com/lin-justin/sarcasm-detection/master/Sarcasm_Headlines_Dataset_v2.json\"\n",
    "\n",
    "# need to massage the text to put it into proper json format\n",
    "with urllib.request.urlopen(url) as url_json:\n",
    "    json_text = \"[\"\n",
    "\n",
    "    for line in url_json:\n",
    "        line_string = line.decode()\n",
    "#        print(\"got line with type: {}\".format(type(line_string))\n",
    "        json_text = json_text + line_string + \",\"\n",
    "\n",
    "    json_text = json_text[:-1] + \"]\"\n",
    "    file_data = json.loads(json_text)\n",
    "    print(\"got data of type: {}\".format(type(url_json)))\n",
    "    print(\"got json data of type: {}\".format(type(json_text)))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "got 28619 elements in my training data\n"
    }
   ],
   "source": [
    "# parse the json\n",
    "sentence_list = []\n",
    "label_list = []\n",
    "url_list = []\n",
    "\n",
    "for item in file_data:\n",
    "    sentence_list.append(item['headline'])\n",
    "    label_list.append(item['is_sarcastic'])\n",
    "    url_list.append(item['article_link'])\n",
    "\n",
    "print(\"got {} elements in my training data\".format(len(sentence_list)))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_print(word_index):\n",
    "    # reverse the word index\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "    # print\n",
    "    for i in range(0, 10):\n",
    "        print(\"the word at position {} is: {}\".format(i, reverse_word_index.get(i, \"?\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "got word index of size: 30885\nthe word at position 0 is: <pad>\nthe word at position 1 is: <OOV>\nthe word at position 2 is: to\nthe word at position 3 is: of\nthe word at position 4 is: the\nthe word at position 5 is: in\nthe word at position 6 is: for\nthe word at position 7 is: a\nthe word at position 8 is: on\nthe word at position 9 is: and\n"
    }
   ],
   "source": [
    "# tokenize the text data for the NN\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenize\n",
    "tokenizer = Tokenizer(oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(sentence_list)\n",
    "\n",
    "# get the word index\n",
    "word_index = tokenizer.word_index\n",
    "print(\"got word index of size: {}\".format(len(word_index)))\n",
    "\n",
    "# add in space\n",
    "word_index['<pad>'] = 0\n",
    "\n",
    "reverse_print(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the first sequence is: [16004   355  3167  7474  2644     3   661  1119     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0]\nthe shape of the padded seuqnces is: (28619, 152) with type: <class 'numpy.ndarray'>\n"
    }
   ],
   "source": [
    "# get the sentence word sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentence_list)\n",
    "\n",
    "# pad tomake sure all row sequnces have the same length and print\n",
    "padded_sequences = pad_sequences(sequences, value= word_index[\"<pad>\"], padding = 'post')\n",
    "print(\"the first sequence is: {}\".format(padded_sequences[0]))\n",
    "print(\"the shape of the padded seuqnces is: {} with type: {}\".format(padded_sequences.shape, type(padded_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the train row index is: 24326\n"
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "# specify training split percent\n",
    "train_split = 0.85\n",
    "\n",
    "# get the index for the split\n",
    "train_index = int(train_split * padded_sequences.shape[0])\n",
    "print(\"the train row index is: {}\".format(train_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "split data into train of size 24326 and test of size 4293\n"
    }
   ],
   "source": [
    "# split into train and test\n",
    "feature_train = padded_sequences[0: train_index]\n",
    "label_train = label_list[0: train_index]\n",
    "feature_test = padded_sequences[train_index:]\n",
    "label_test = label_list[train_index:]\n",
    "\n",
    "print(\"split data into train of size {} and test of size {}\".format(len(label_train), len(label_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the train sentences word index is of size 28488\n"
    }
   ],
   "source": [
    "# now need to create a new training tokenizer fit only on the traim data\n",
    "tokenizer_train = Tokenizer(oov_token='<OOV\\>', num_words=10000)\n",
    "\n",
    "# split the train and test senetences\n",
    "sentence_train = sentence_list[:train_index]\n",
    "sentence_test = sentence_list[train_index:]\n",
    "\n",
    "# train this tokenizer on the training features\n",
    "tokenizer_train.fit_on_texts(sentence_train)\n",
    "\n",
    "train_word_index = tokenizer_train.word_index\n",
    "print(\"the train sentences word index is of size {}\".format(len(train_word_index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the train padded sequence is of shape (24326, 120) and the test padded sequence is of shape (4293, 120)\n"
    }
   ],
   "source": [
    "# now get the sequences for the train and test sentence lists\n",
    "train_sequence = tokenizer_train.texts_to_sequences(sentence_train)\n",
    "test_sequence = tokenizer_train.texts_to_sequences(sentence_test)\n",
    "\n",
    "# pad (TODO: parameterize the constants)\n",
    "train_sequence_padded = pad_sequences(train_sequence, padding='post', truncating='post', maxlen=120)\n",
    "test_sequence_padded = pad_sequences(test_sequence, padding='post', truncating='post', maxlen=120)\n",
    "\n",
    "# make sure the shapes match on the column dimension (needed for the NN)\n",
    "print(\"the train padded sequence is of shape {} and the test padded sequence is of shape {}\".format(train_sequence_padded.shape, test_sequence_padded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the word index is of type <class 'dict'> and size 30886\n('<OOV>', 1)\n('to', 2)\n('of', 3)\n('the', 4)\n('in', 5)\n('for', 6)\n('a', 7)\n('on', 8)\n('and', 9)\n('with', 10)\nthe word at position 0 is: ?\nthe word at position 1 is: <OOV\\>\nthe word at position 2 is: to\nthe word at position 3 is: of\nthe word at position 4 is: the\nthe word at position 5 is: in\nthe word at position 6 is: for\nthe word at position 7 is: a\nthe word at position 8 is: on\nthe word at position 9 is: and\n"
    }
   ],
   "source": [
    "print(\"the word index is of type {} and size {}\".format(type(train_word_index), len(word_index)))\n",
    "\n",
    "iterator = iter(word_index.items())\n",
    "for i in range(10):\n",
    "    print(next(iterator))\n",
    "\n",
    "reverse_print(train_word_index)\n",
    "# print(train_word_index[\"<pad>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_6 (Embedding)      (None, None, 16)          160000    \n_________________________________________________________________\nglobal_average_pooling1d_6 ( (None, 16)                0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 16)                272       \n_________________________________________________________________\ndense_13 (Dense)             (None, 1)                 17        \n=================================================================\nTotal params: 160,289\nTrainable params: 160,289\nNon-trainable params: 0\n_________________________________________________________________\nthe tf model summary is None\n"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# build the NN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "print(\"the tf model summary is {}\".format(model.summary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train sequence has type <class 'numpy.ndarray'> and shape (24326, 120)\ntrain sequence numpy has type <class 'numpy.ndarray'> and shape (24326, 120)\ntrain labels has type <class 'list'> and shape 24326\ntrain labels numpy has type <class 'numpy.ndarray'> and shape (24326,)\n"
    }
   ],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "training_padded_np = np.array(train_sequence_padded)\n",
    "training_labels_np = np.array(label_train)\n",
    "testing_padded_np = np.array(test_sequence_padded)\n",
    "testing_labels_np = np.array(label_test)\n",
    "\n",
    "print(\"train sequence has type {} and shape {}\".format(type(train_sequence_padded), train_sequence_padded.shape))\n",
    "print(\"train sequence numpy has type {} and shape {}\".format(type(training_padded_np), training_padded_np.shape))\n",
    "\n",
    "print(\"train labels has type {} and shape {}\".format(type(label_train), len(label_train)))\n",
    "print(\"train labels numpy has type {} and shape {}\".format(type(training_labels_np), training_labels_np.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train sequence has type <class 'numpy.ndarray'> and length (24326, 120)\n[   1  321 3400 6636 2414    3  662 1013    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0]\n train label has type <class 'list'> and length 24326\n[0, 0, 1, 1, 0, 0, 1, 1, 0]\n"
    }
   ],
   "source": [
    "print(\" train sequence has type {} and length {}\".format(type(train_sequence_padded), train_sequence_padded.shape))\n",
    "print(train_sequence_padded[0])\n",
    "print(\" train label has type {} and length {}\".format(type(label_train), len(label_train)))\n",
    "print(label_train[1:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 24326 samples, validate on 4293 samples\nEpoch 1/20\n24326/24326 [==============================] - 1s 46us/sample - loss: 0.0064 - accuracy: 0.9980 - val_loss: 2.5328 - val_accuracy: 0.7866\nEpoch 2/20\n24326/24326 [==============================] - 1s 47us/sample - loss: 0.0055 - accuracy: 0.9983 - val_loss: 2.5892 - val_accuracy: 0.7871\nEpoch 3/20\n24326/24326 [==============================] - 1s 46us/sample - loss: 0.0049 - accuracy: 0.9988 - val_loss: 2.5847 - val_accuracy: 0.7866\nEpoch 4/20\n24326/24326 [==============================] - 1s 47us/sample - loss: 0.0050 - accuracy: 0.9986 - val_loss: 2.5493 - val_accuracy: 0.7857\nEpoch 5/20\n24326/24326 [==============================] - 1s 47us/sample - loss: 0.0050 - accuracy: 0.9988 - val_loss: 2.6938 - val_accuracy: 0.7850\nEpoch 6/20\n24326/24326 [==============================] - 1s 46us/sample - loss: 0.0047 - accuracy: 0.9987 - val_loss: 2.5470 - val_accuracy: 0.7834\nEpoch 7/20\n24326/24326 [==============================] - 1s 46us/sample - loss: 0.0051 - accuracy: 0.9986 - val_loss: 2.8331 - val_accuracy: 0.7843\nEpoch 8/20\n24326/24326 [==============================] - 1s 46us/sample - loss: 0.0058 - accuracy: 0.9984 - val_loss: 2.6121 - val_accuracy: 0.7848\nEpoch 9/20\n24326/24326 [==============================] - 1s 47us/sample - loss: 0.0050 - accuracy: 0.9986 - val_loss: 2.4452 - val_accuracy: 0.7871\nEpoch 10/20\n24326/24326 [==============================] - 1s 48us/sample - loss: 0.0037 - accuracy: 0.9992 - val_loss: 2.6663 - val_accuracy: 0.7857\nEpoch 11/20\n24326/24326 [==============================] - 1s 51us/sample - loss: 0.0040 - accuracy: 0.9989 - val_loss: 2.4944 - val_accuracy: 0.7869\nEpoch 12/20\n24326/24326 [==============================] - 1s 47us/sample - loss: 0.0039 - accuracy: 0.9991 - val_loss: 2.8024 - val_accuracy: 0.7845\nEpoch 13/20\n24326/24326 [==============================] - 1s 52us/sample - loss: 0.0052 - accuracy: 0.9984 - val_loss: 2.7540 - val_accuracy: 0.7857\nEpoch 14/20\n24326/24326 [==============================] - 1s 48us/sample - loss: 0.0045 - accuracy: 0.9988 - val_loss: 2.7499 - val_accuracy: 0.7857\nEpoch 15/20\n24326/24326 [==============================] - 1s 49us/sample - loss: 0.0035 - accuracy: 0.9992 - val_loss: 2.9095 - val_accuracy: 0.7836\nEpoch 16/20\n24326/24326 [==============================] - 1s 47us/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 2.7757 - val_accuracy: 0.7850\nEpoch 17/20\n24326/24326 [==============================] - 1s 48us/sample - loss: 0.0051 - accuracy: 0.9984 - val_loss: 2.6844 - val_accuracy: 0.7859\nEpoch 18/20\n24326/24326 [==============================] - 1s 48us/sample - loss: 0.0042 - accuracy: 0.9988 - val_loss: 2.7548 - val_accuracy: 0.7862\nEpoch 19/20\n24326/24326 [==============================] - 1s 48us/sample - loss: 0.0037 - accuracy: 0.9990 - val_loss: 2.6971 - val_accuracy: 0.7848\nEpoch 20/20\n24326/24326 [==============================] - 1s 49us/sample - loss: 0.0040 - accuracy: 0.9990 - val_loss: 2.9242 - val_accuracy: 0.7836\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fad399fa810>"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "# train the tf model\n",
    "num_epochs = 20\n",
    "\n",
    "model.fit(training_padded_np, training_labels_np, epochs= num_epochs, validation_data= (testing_padded_np, testing_labels_np), verbose= 1)\n",
    "# model.fit(train_sequence_padded, training_labels_np, epochs= num_epochs, validation_data= (test_sequence_padded, testing_labels_np), verbose= 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittf237venv9b274482c7ba4966ad2cf02baa9bb24c",
   "display_name": "Python 3.7.6 64-bit ('tf2_37': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}