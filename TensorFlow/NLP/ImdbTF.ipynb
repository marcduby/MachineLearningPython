{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "TF Version:  2.1.0\nnumpy version 1.18.1\nEager mode:  True\nGPU is NOT AVAILABLE\n"
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# print environment\n",
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"numpy version {}\".format(np.__version__))\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "# print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The imdb data is of type <class 'module'>\n"
    }
   ],
   "source": [
    "# get the imdb data\n",
    "imdb_data = keras.datasets.imdb\n",
    "\n",
    "# print\n",
    "print(\"The imdb data is of type {}\".format(type(imdb_data)))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data\n",
    "(train_data, train_label), (test_data, test_label) = imdb_data.load_data(num_words= 10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the train_data is shape (25000,) and train_label is of shape (25000,)\nthe test_data is shape (25000,) and test_label is of shape (25000,)\nthe first row of the training data [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\nthe first row of the training data is of length 218\nthe first row of the training label 1\n"
    }
   ],
   "source": [
    "# print\n",
    "print(\"the train_data is shape {} and train_label is of shape {}\".format(train_data.shape, train_label.shape))\n",
    "print(\"the test_data is shape {} and test_label is of shape {}\".format(test_data.shape, test_label.shape))\n",
    "\n",
    "# print\n",
    "print(\"the first row of the training data {}\".format(train_data[0]))\n",
    "print(\"the first row of the training data is of length {}\".format(len(train_data[0])))\n",
    "print(\"the first row of the training label {}\".format(train_label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the word in is of shape 88584\n"
    }
   ],
   "source": [
    "# get the word index\n",
    "word_index = imdb_data.get_word_index()\n",
    "\n",
    "# print\n",
    "print(\"the word in is of shape {}\".format(len(word_index)))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the word index is of type <class 'dict'> and size 88585\n('fawn', 34702)\n('tsukino', 52007)\n('nunnery', 52008)\n('sonja', 16817)\n('vani', 63952)\n('woods', 1409)\n('spiders', 16116)\n('hanging', 2346)\n('woody', 2290)\n('trawling', 52009)\n"
    }
   ],
   "source": [
    "print(\"the word index is of type {} and size {}\".format(type(word_index), len(word_index)))\n",
    "\n",
    "iterator = iter(word_index.items())\n",
    "for i in range(10):\n",
    "    print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the word in is of shape 88585\n"
    }
   ],
   "source": [
    "# add in the unknown word\n",
    "word_index = {key: (value+1) for key, value in word_index.items()}\n",
    "\n",
    "# add in space\n",
    "word_index['<pad>'] = 0\n",
    "\n",
    "# print\n",
    "print(\"the word in is of shape {}\".format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the word at position 0 is: <pad>\nthe word at position 1 is: ?\nthe word at position 2 is: the\nthe word at position 3 is: and\nthe word at position 4 is: a\nthe word at position 5 is: of\nthe word at position 6 is: to\nthe word at position 7 is: is\nthe word at position 8 is: br\nthe word at position 9 is: in\n"
    }
   ],
   "source": [
    "# reverse the word index\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# print\n",
    "for i in range(0, 10):\n",
    "    print(\"the word at position {} is: {}\".format(i, reverse_word_index.get(i, \"?\")))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "? a the the an julie a lucky certainly life thing characters a murders that me here had i which the within who mitchell add which lois beginning leaps a create is the my look a two isn't immediately just player in as is who within africa at br a four you a the playwright to duke about like her show my direction it it is looking this daughter life thing didn't my for bar on his murder totally her a them my jack this a again see in living i wendigo a therefore of the is milk with that introducing this but to despite at everyone i the to bring save i br original he high a mel insulting br desperately even it it girls stereotypes world or was person every is course plot from old stupid a girls course lee living down up been well fine so person course sympathy pretty of with been claim fi so sense here locked with wave about with who realize the big of audiences recognized but the saw carl davis girls the surprisingly of needs it it which style in the or which relation of he relation didn't don't course rose solo audience br down of people solo a see as marx real back resemble this tickets girls animation girls girls as automatic a the ago is milk way honeymoon versus the i the a obviously is passion tv the in the i i'll was in more itself this way half impression this some to examples america of longest the but what is a aware the concept his also better sequences it it the the 1983 he relation i 90 twin didn't he used one summary but goodness every is don't the br all i a wanting because 15 br destruction if i the on one in children's you a missing the comments to the loves superman it it tv good in to facial humanity animation don't one the br ever a dr a dr art he the reviewer girls show your audiences man of one behind br ever a dr art a used is he lee it it one in below br too to might the chase br a the is at is the made the versus me a dr all living about in this by i girls it it i was see out girls been the holds briefly the taylor a the is actors of a gave very the the is portrait dr i keep like been anyone unbearable but the with one direction a see is he i've never it it the a the the in am a made questions the of references himself not is milk better then or the that however convincing attraction but a the is bodies girls you very girls on people or the that the was reason that interesting a the worse mention sexual a unique ended the is ripping of way or his he's would the a the sorry the is a frenchman these br had to not as in everything br milk was at in am hands there his sense the you this includes thought his he's would avoid the a room is julie of a moment actors street is moving good have what steer as police all living on really subtitles\nthe label for this review is 1\n"
    }
   ],
   "source": [
    "# method to decode text\n",
    "def decode_text(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])\n",
    "\n",
    "# print a review\n",
    "index = 3\n",
    "print(decode_text(train_data[index]))\n",
    "print(\"the label for this review is {}\".format(train_label[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "? was some br all at is a day seen is a bridges up which truly that well though was not plot a offensive house this for doo an been it's england american a because wife who thinking but course 3d terms a spanish an most will this their for a kids is a even special father i a witch it's ridiculous documentary br anyone great honest about steve sorts other can craft as so director land once so were the york br films was hoot soldiers movie to not this role be number or to how black you vote on you not this book or she at i a not some to facts more in same you brother best brother batman that take do jokes most the was in br films hell released who thinking to script is still never <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\nthe label for this review is 0\nthe train data is of shape (25000, 300)  and type <class 'numpy.ndarray'> and the test data is of shape (25000, 300) and type <class 'numpy.ndarray'>\n"
    }
   ],
   "source": [
    "# preprocess the text\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value= word_index[\"<pad>\"], padding= \"post\", maxlen= 300)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value= word_index[\"<pad>\"], padding= \"post\", maxlen = 300)\n",
    "\n",
    "# print a review\n",
    "index = 2\n",
    "print(decode_text(train_data[index]))\n",
    "print(\"the label for this review is {}\".format(train_label[index]))\n",
    "print(\"the train data is of shape {}  and type {} and the test data is of shape {} and type {}\".format(train_data.shape, type(train_data), test_data.shape, type(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 16)          160000    \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 160,289\nTrainable params: 160,289\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# build the model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(10000, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation= \"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation= \"sigmoid\"))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the train is of size (21000, 300) and the validation is of size (4000, 300)\n"
    }
   ],
   "source": [
    "# split out the validation data\n",
    "split_index = 4000\n",
    "X_train = train_data[split_index:]\n",
    "X_validation = train_data[:split_index]\n",
    "y_train = train_label[split_index:]\n",
    "y_validation = train_label[:split_index]\n",
    "\n",
    "# print\n",
    "print(\"the train is of size {} and the validation is of size {}\".format(X_train.shape, X_validation.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 21000 samples, validate on 4000 samples\nEpoch 1/40\n21000/21000 [==============================] - 1s 32us/sample - loss: 0.6917 - accuracy: 0.5263 - val_loss: 0.6891 - val_accuracy: 0.7352\nEpoch 2/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.6842 - accuracy: 0.7272 - val_loss: 0.6773 - val_accuracy: 0.7107\nEpoch 3/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.6663 - accuracy: 0.7274 - val_loss: 0.6529 - val_accuracy: 0.7290\nEpoch 4/40\n21000/21000 [==============================] - 0s 16us/sample - loss: 0.6345 - accuracy: 0.7423 - val_loss: 0.6162 - val_accuracy: 0.7613\nEpoch 5/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.5901 - accuracy: 0.7904 - val_loss: 0.5688 - val_accuracy: 0.8010\nEpoch 6/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.5343 - accuracy: 0.8305 - val_loss: 0.5141 - val_accuracy: 0.8342\nEpoch 7/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.4765 - accuracy: 0.8539 - val_loss: 0.4628 - val_accuracy: 0.8472\nEpoch 8/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.4239 - accuracy: 0.8691 - val_loss: 0.4201 - val_accuracy: 0.8555\nEpoch 9/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.3811 - accuracy: 0.8779 - val_loss: 0.3881 - val_accuracy: 0.8637\nEpoch 10/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.3453 - accuracy: 0.8890 - val_loss: 0.3625 - val_accuracy: 0.8712\nEpoch 11/40\n21000/21000 [==============================] - 0s 16us/sample - loss: 0.3177 - accuracy: 0.8951 - val_loss: 0.3420 - val_accuracy: 0.8730\nEpoch 12/40\n21000/21000 [==============================] - 0s 16us/sample - loss: 0.2938 - accuracy: 0.9006 - val_loss: 0.3272 - val_accuracy: 0.8742\nEpoch 13/40\n21000/21000 [==============================] - 0s 16us/sample - loss: 0.2750 - accuracy: 0.9055 - val_loss: 0.3162 - val_accuracy: 0.8765\nEpoch 14/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.2588 - accuracy: 0.9107 - val_loss: 0.3075 - val_accuracy: 0.8817\nEpoch 15/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.2452 - accuracy: 0.9143 - val_loss: 0.3015 - val_accuracy: 0.8808\nEpoch 16/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.2327 - accuracy: 0.9198 - val_loss: 0.2946 - val_accuracy: 0.8838\nEpoch 17/40\n21000/21000 [==============================] - 0s 14us/sample - loss: 0.2216 - accuracy: 0.9230 - val_loss: 0.2908 - val_accuracy: 0.8827\nEpoch 18/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.2115 - accuracy: 0.9274 - val_loss: 0.2882 - val_accuracy: 0.8802\nEpoch 19/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.2037 - accuracy: 0.9299 - val_loss: 0.2860 - val_accuracy: 0.8813\nEpoch 20/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1964 - accuracy: 0.9337 - val_loss: 0.2842 - val_accuracy: 0.8863\nEpoch 21/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1877 - accuracy: 0.9373 - val_loss: 0.2844 - val_accuracy: 0.8808\nEpoch 22/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1810 - accuracy: 0.9398 - val_loss: 0.2826 - val_accuracy: 0.8823\nEpoch 23/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1743 - accuracy: 0.9415 - val_loss: 0.2822 - val_accuracy: 0.8865\nEpoch 24/40\n21000/21000 [==============================] - 0s 16us/sample - loss: 0.1706 - accuracy: 0.9435 - val_loss: 0.2820 - val_accuracy: 0.8835\nEpoch 25/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1634 - accuracy: 0.9462 - val_loss: 0.2829 - val_accuracy: 0.8848\nEpoch 26/40\n21000/21000 [==============================] - 0s 16us/sample - loss: 0.1587 - accuracy: 0.9478 - val_loss: 0.2819 - val_accuracy: 0.8865\nEpoch 27/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1534 - accuracy: 0.9504 - val_loss: 0.2832 - val_accuracy: 0.8863\nEpoch 28/40\n21000/21000 [==============================] - 0s 14us/sample - loss: 0.1488 - accuracy: 0.9520 - val_loss: 0.2861 - val_accuracy: 0.8848\nEpoch 29/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1446 - accuracy: 0.9530 - val_loss: 0.2850 - val_accuracy: 0.8870\nEpoch 30/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1400 - accuracy: 0.9549 - val_loss: 0.2870 - val_accuracy: 0.8882\nEpoch 31/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1361 - accuracy: 0.9561 - val_loss: 0.2871 - val_accuracy: 0.8873\nEpoch 32/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1317 - accuracy: 0.9588 - val_loss: 0.2914 - val_accuracy: 0.8890\nEpoch 33/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1308 - accuracy: 0.9576 - val_loss: 0.2907 - val_accuracy: 0.8870\nEpoch 34/40\n21000/21000 [==============================] - 0s 14us/sample - loss: 0.1248 - accuracy: 0.9608 - val_loss: 0.2918 - val_accuracy: 0.8880\nEpoch 35/40\n21000/21000 [==============================] - 0s 14us/sample - loss: 0.1211 - accuracy: 0.9630 - val_loss: 0.2933 - val_accuracy: 0.8875\nEpoch 36/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1174 - accuracy: 0.9639 - val_loss: 0.2972 - val_accuracy: 0.8863\nEpoch 37/40\n21000/21000 [==============================] - 0s 14us/sample - loss: 0.1158 - accuracy: 0.9638 - val_loss: 0.3005 - val_accuracy: 0.8850\nEpoch 38/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1119 - accuracy: 0.9662 - val_loss: 0.3011 - val_accuracy: 0.8873\nEpoch 39/40\n21000/21000 [==============================] - 0s 16us/sample - loss: 0.1081 - accuracy: 0.9676 - val_loss: 0.3027 - val_accuracy: 0.8873\nEpoch 40/40\n21000/21000 [==============================] - 0s 15us/sample - loss: 0.1052 - accuracy: 0.9686 - val_loss: 0.3069 - val_accuracy: 0.8873\n25000/25000 [==============================] - 0s 20us/sample - loss: 0.3281 - accuracy: 0.8755\nthe evaluation results are [0.32806218278884886, 0.87548]\n"
    }
   ],
   "source": [
    "# fit the model\n",
    "model_fit = model.fit(X_train, y_train, epochs = 40, batch_size = 512, validation_data = (X_validation, y_validation), verbose = 1)\n",
    "\n",
    "# evaluate the fit\n",
    "result = model.evaluate(test_data, test_label)\n",
    "\n",
    "print(\"the evaluation results are {}\".format(result))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittf237venv9b274482c7ba4966ad2cf02baa9bb24c",
   "display_name": "Python 3.7.6 64-bit ('tf2_37': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}