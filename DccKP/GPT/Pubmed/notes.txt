
- VM setup
  - run: 'source activate pytorch'
  - monitor: 


- approach
  - need context, then lists of q/a 
  - use abstract as context 
  - generate q/a from keywords, with determining type (gene/pheno) from node normalizer 
  - then run through all abstracts and generate stats 


- notice
  - have access to references 
    - good for translator

20230430 - nvidia gpu aws
Every 5.0s: nvidia-smi                                                                                                        ip-172-31-17-134: Sun Apr 30 16:44:11 2023

Sun Apr 30 16:44:11 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |
|  0%   51C    P0   263W / 300W |  12858MiB / 23028MiB |     99%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     33591      C   python3                         12856MiB |
+-----------------------------------------------------------------------------+


=============================================================================
       __|  __|_  )
       _|  (     /   Deep Learning AMI GPU PyTorch 2.0.0 (Ubuntu 20.04)
      ___|\___|___|
=============================================================================

Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-1033-aws x86_64v)

* Please note that Amazon EC2 P2 Instance is not supported on current DLAMI.
* Supported EC2 instances: G3, P3, P3dn, P4d, P4de, G5, G4dn.
* To activate pre-built pytorch environment, run: 'source activate pytorch'
* To activate base conda environment upon login, run: 'conda config --set auto_activate_base true'
* NVIDIA driver version: 525.85.12
* CUDA version: 11.8 



20230428 - 64 stream, 80 input tokens, 

 24%|██▍       | 6/25 [1:00:51<3:12:48, 608.88s/it]
<start> diabetes is associated with genes <bot>: and it is associated with the immune system. We also find that the level of the immune system is related to the level of immune infiltration and the level of the immune system. Finally <end>                              
<start> PCSK9 is a gene <bot>: which is essential for the proper formation of the mitochondrial membrane. Here <end>                                                                            
<start> diabetes is a disease <bot>: and it is often caused by genetic or epigenetic factors. We aim to identify the genetic causes of the common and rare <end>                                                        
 28%|██▊       | 7/25 [1:11:00<3:02:40, 608.92s/it]
 <start> diabetes is associated with genes <bot>: and the association between the SEP and DNAm was analyzed. The SEP explained a significant proportion of the phenotypic variation. The SEP explained a significant proportion of the phenotypic variation. The SEP explained a significant proportion of the phenotyp
<start> PCSK9 is a gene <bot>: which is essential for the proper formation of the proper protein. Here <end>                                                                            
<start> diabetes is a disease <bot>: and the relationship between genetic and environmental conditions has not been studied. In this study <end>                                                                        
 32%|███▏      | 8/25 [1:21:09<2:52:30, 608.85s/it]
 <start> diabetes is associated with genes <bot>: and the association between the miR-494-3p and miR-544a was close to that observed for all DE traits. In addition <end>                                          
<start> PCSK9 is a gene <bot>: which is tightly correlated with immune infiltration and may therefore be an independent predictive factor. Therefore <end>                                                                    
<start> diabetes is a disease <bot>: and the use of big data in medicine is underrepresented among patients in cardiology. We aimed to explore the perspectives and potential implications of healthcare professionals in providing genetics counseling and testing to all patients. Semi-structured interviews were conducted with 10 healthcare professionals from four clinical
 36%|███▌      | 9/25 [1:31:17<2:42:18, 608.67s/it]
 <start> diabetes is associated with genes <bot>: and the use of big data in medicine may improve practice in the aging sample. <end>                                                                      
<start> PCSK9 is a gene <bot>: and its origin often explained by biological processes such as taste <bot>: and others. We identified eight wayfinding-based methods for gene discovery of important environmental adaptation factors: the simple genomic models from blocks with and those from moderately and highly complex regions by feature selection. For
<start> diabetes is a disease <bot>: and its incident incidence rate when 14q11.2 duplication was identified in a cohort of 6049 respondents who were BRCA <end>                                                    
 40%|████      | 10/25 [1:41:25<2:32:09, 608.65s/it]
 <start> diabetes is associated with genes <bot>: which are either similar or discordant. The present study identified a possible association between age and alcohol consumption and the risk of ESCC. The findings also revealed a significant association between alcohol consumption and ESCC <end>                      
<start> PCSK9 is a gene <bot>: which usually occurs in isolation from other animals. The SNP23 recombinant inbred lines (RILs) developed inbred lines with the additive RILs in two populations. The RILs in the parental genome were ranged from 0.4 to 0
<start> diabetes is a disease <bot>: and the results were confirmed through the functional tests <end>                                                                                      
 44%|████▍     | 11/25 [1:51:35<2:22:03, 608.79s/it]
 <start> diabetes is associated with genes <bot>: and scoliosis may be used as a risk factor to stroke.  We investigated the association between the IFN-γ expression and the risk of stroke. We found that the expression of the inflammatory aging clock-related genes was strongly correlated with the development of
<start> PCSK9 is a gene <bot>: which is highly expressed in neurons. We previously reported that mice with the MPZD61N variant <end>                                                              
<start> diabetes is a disease <bot>: and the association between miRNA-124 expression and DNA methylation in the three groups of this study. The results of the current study revealed that miR-124 can be a novel target for the treatment of type 2 diabetes mellitus (T2DM) <end>
 48%|████▊     | 12/25 [2:01:44<2:11:58, 609.13s/it]
 <start> diabetes is associated with genes <bot>: and the association between miRNA-124 and scoliosis is elusive. Therefore <end>                                                                      
<start> PCSK9 is a gene <bot>: and the mechanism of this activation is related to host metabolism and is probably related to host activation <end>                                                                  
<start> diabetes is a disease <bot>: and the common mechanism of the most common type of diabetes is based on the genetic predisposition to SLE. The genetic predisposition to SLE development was confirmed in most studies but not in the many studies. The approach of the study was differentially expressed into three
 52%|█████▏    | 13/25 [2:11:54<2:01:52, 609.40s/it]
 <start> diabetes is associated with genes <bot>: and the common comorbidity outcome is stroke and chronic depression. <end>                                                                          
<start> PCSK9 is a gene <bot>: which is directly regulated by the Dfd and ScrY gene. The same gene expression pattern in the three selected P1D mutant mice is different from that in the control group. The p1D mutant shows an overall proliferation and low survival rate compared to
<start> diabetes is a disease <bot>: and its occurrence has been documented in the past few decades. In this regard <end>                                                                          
 56%|█████▌    | 14/25 [2:22:03<1:51:42, 609.29s/it]


20230427 - training with 110k lines and length of 80
training .... 
  0%|          | 0/12 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  8%|▊         | 1/12 [03:42<40:50, 222.79s/it]<startofstring> PCSK9 is a gene <bot>: and a gene-related gene gene gene
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 17%|█▋        | 2/12 [07:25<37:04, 222.48s/it]<startofstring> PCSK9 is a gene <bot>: which is associated with the expression of the
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 25%|██▌       | 3/12 [11:08<33:24, 222.70s/it]<startofstring> PCSK9 is a gene <bot>: which is associated with the occurrence of various
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 33%|███▎      | 4/12 [14:51<29:42, 222.86s/it]<startofstring> PCSK9 is a gene <bot>: and the prognostic value of the prog
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 42%|████▏     | 5/12 [18:33<25:59, 222.84s/it]<startofstring> PCSK9 is a gene <bot>: and is associated with the occurrence of a
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 50%|█████     | 6/12 [22:17<22:18, 223.07s/it]<startofstring> PCSK9 is a gene <bot>: and it is involved in the regulation of
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 58%|█████▊    | 7/12 [26:00<18:35, 223.01s/it]<startofstring> PCSK9 is a gene <bot>: and prognostic value is validated in the
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 67%|██████▋   | 8/12 [29:43<14:51, 222.98s/it]<startofstring> PCSK9 is a gene <bot>: and prognosticnostic prediction is a new
 67%|██████▋   | 8/12 [31:32<15:46, 236.53s/it]
 



from chatgpt:
Marc Duby
explain how to build a training dataset to fine tune transformer question answer model

Building a training dataset to fine-tune a transformer question-answer model typically involves the following steps:

Define the task: First, you need to define the task that your model will perform. In this case, the task is question-answering, where the model takes a question as input and produces an answer.

Choose a dataset: Next, you need to choose a dataset that is suitable for the task. Some popular question-answering datasets include SQuAD (Stanford Question Answering Dataset), MS MARCO (Microsoft MAchine Reading COmprehension), and TriviaQA.

Preprocess the dataset: Once you have chosen a dataset, you will need to preprocess it to prepare it for training. This may involve converting the data into a format that can be ingested by your model, such as converting it into JSON or CSV format.

Create training examples: For each question-answer pair in the dataset, you will need to create a training example that includes the question, the answer, and any relevant context. This context might include the paragraph or article that the question is taken from, for example.

Augment the dataset: To improve the performance of your model, you may want to augment the dataset by generating additional training examples. This can be done through techniques such as data augmentation or paraphrasing.
training .... 
  0%|          | 0/12 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  8%|▊         | 1/12 [03:42<40:50, 222.79s/it]<startofstring> PCSK9 is a gene <bot>: and a gene-related gene gene gene
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 17%|█▋        | 2/12 [07:25<37:04, 222.48s/it]<startofstring> PCSK9 is a gene <bot>: which is associated with the expression of the
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 25%|██▌       | 3/12 [11:08<33:24, 222.70s/it]<startofstring> PCSK9 is a gene <bot>: which is associated with the occurrence of various
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 33%|███▎      | 4/12 [14:51<29:42, 222.86s/it]<startofstring> PCSK9 is a gene <bot>: and the prognostic value of the prog
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 42%|████▏     | 5/12 [18:33<25:59, 222.84s/it]<startofstring> PCSK9 is a gene <bot>: and is associated with the occurrence of a
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 50%|█████     | 6/12 [22:17<22:18, 223.07s/it]<startofstring> PCSK9 is a gene <bot>: and it is involved in the regulation of
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 58%|█████▊    | 7/12 [26:00<18:35, 223.01s/it]<startofstring> PCSK9 is a gene <bot>: and prognostic value is validated in the
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 67%|██████▋   | 8/12 [29:43<14:51, 222.98s/it]<startofstring> PCSK9 is a gene <bot>: and prognosticnostic prediction is a new
 67%|██████▋   | 8/12 [31:32<15:46, 236.53s/it]
Split the dataset: Finally, you will need to split the dataset into training, validation, and testing sets. The training set will be used to train the model, the validation set will be used to tune hyperparameters, and the testing set will be used to evaluate the final performance of the model.

By following these steps, you can create a training dataset that can be used to fine-tune a transformer question-answer model. Once you have a suitable dataset, you can use it to train your model using techniques such as transfer learning or fine-tuning.




Marc Duby
can I then ask free form questions, or do I need to provide a context for each question
In general, it is better to provide context for each question when using a transformer question-answer model. This is because transformer models are trained to perform question-answering on specific contexts, such as a particular article or paragraph. Without context, the model may not be able to accurately understand the question and provide a relevant answer.

However, some transformer models are designed to work without explicit context. These models, such as GPT-3, are trained on a diverse set of language tasks and can generate high-quality responses to a wide variety of questions without explicit context.

So, whether you need to provide context for each question or not depends on the specific transformer model you are using and the task you are trying to perform. If you are unsure, it is a good idea to provide context to ensure the best possible performance.




Marc Duby
what type of question answering open source transformer model requires no context
One example of a transformer-based open source question-answering model that requires no explicit context is OpenAI's GPT-3 (Generative Pre-trained Transformer 3) model.

GPT-3 is a large-scale transformer model trained on a diverse range of natural language processing tasks, including question-answering. It is designed to be a "plug-and-play" language model, meaning that it can be used for a wide range of natural language tasks, including question-answering, without requiring explicit context.

When given a question, GPT-3 generates a response based on its understanding of natural language and its training on a large corpus of text. GPT-3 is able to generate high-quality responses to a wide variety of questions, even if no explicit context is provided.

That being said, it's important to note that GPT-3 may not always provide accurate or relevant answers to all types of questions. It is still important to carefully evaluate the quality of the answers generated by GPT-3, especially when using it for critical applications such as medical diagnosis or legal advice.




Marc Duby
do any gpt-2 models provide question answer capability without context
No, the standard GPT-2 models do not provide question-answer capability without context. The GPT-2 model is a language model that generates text based on the input context, but it is not specifically designed for question-answering.

However, there have been some research efforts to modify the GPT-2 model for question-answering tasks. For example, the GPT-2 model can be fine-tuned on question-answering datasets such as SQuAD or TriviaQA, which allows it to generate answers to specific questions. But, in these cases, the model still requires a context that includes the passage or document from which the question is being asked.

In summary, while the GPT-2 model can be fine-tuned for question-answering tasks, it is not designed for this purpose and cannot provide accurate answers without an input context. For question-answering tasks without explicit context, models like GPT-3 or other specialized question-answering models might be more suitable.




Marc Duby
does databricks dolly model need context to answer questions
Yes, Databricks Dolly model, like most question-answering models, typically requires context to provide accurate answers to questions. Dolly is a transformer-based language model that has been fine-tuned on a variety of natural language tasks, including question-answering.

When given a question, Dolly uses its understanding of natural language and its training on large amounts of text data to generate a response. However, the quality and accuracy of Dolly's answers depend on the context provided. Without context, Dolly may not be able to accurately understand the question and provide a relevant answer.

That being said, it's important to note that the quality and accuracy of Dolly's answers also depend on the quality of the training data and the specific fine-tuning process used. So, while Dolly may require context to provide accurate answers, the overall performance of the model can be improved by using high-quality training data and appropriate fine-tuning techniques.
