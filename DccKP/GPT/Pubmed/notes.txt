

- approach
  - need context, then lists of q/a 
  - use abstract as context 
  - generate q/a from keywords, with determining type (gene/pheno) from node normalizer 
  - then run through all abstracts and generate stats 


- notice
  - have access to references 
    - good for translator




20230427 - training with 110k lines and length of 80
training .... 
  0%|          | 0/12 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  8%|▊         | 1/12 [03:42<40:50, 222.79s/it]<startofstring> PCSK9 is a gene <bot>: and a gene-related gene gene gene
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 17%|█▋        | 2/12 [07:25<37:04, 222.48s/it]<startofstring> PCSK9 is a gene <bot>: which is associated with the expression of the
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 25%|██▌       | 3/12 [11:08<33:24, 222.70s/it]<startofstring> PCSK9 is a gene <bot>: which is associated with the occurrence of various
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 33%|███▎      | 4/12 [14:51<29:42, 222.86s/it]<startofstring> PCSK9 is a gene <bot>: and the prognostic value of the prog
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 42%|████▏     | 5/12 [18:33<25:59, 222.84s/it]<startofstring> PCSK9 is a gene <bot>: and is associated with the occurrence of a
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 50%|█████     | 6/12 [22:17<22:18, 223.07s/it]<startofstring> PCSK9 is a gene <bot>: and it is involved in the regulation of
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 58%|█████▊    | 7/12 [26:00<18:35, 223.01s/it]<startofstring> PCSK9 is a gene <bot>: and prognostic value is validated in the
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 67%|██████▋   | 8/12 [29:43<14:51, 222.98s/it]<startofstring> PCSK9 is a gene <bot>: and prognosticnostic prediction is a new
 67%|██████▋   | 8/12 [31:32<15:46, 236.53s/it]
 



from chatgpt:
Marc Duby
explain how to build a training dataset to fine tune transformer question answer model

Building a training dataset to fine-tune a transformer question-answer model typically involves the following steps:

Define the task: First, you need to define the task that your model will perform. In this case, the task is question-answering, where the model takes a question as input and produces an answer.

Choose a dataset: Next, you need to choose a dataset that is suitable for the task. Some popular question-answering datasets include SQuAD (Stanford Question Answering Dataset), MS MARCO (Microsoft MAchine Reading COmprehension), and TriviaQA.

Preprocess the dataset: Once you have chosen a dataset, you will need to preprocess it to prepare it for training. This may involve converting the data into a format that can be ingested by your model, such as converting it into JSON or CSV format.

Create training examples: For each question-answer pair in the dataset, you will need to create a training example that includes the question, the answer, and any relevant context. This context might include the paragraph or article that the question is taken from, for example.

Augment the dataset: To improve the performance of your model, you may want to augment the dataset by generating additional training examples. This can be done through techniques such as data augmentation or paraphrasing.
training .... 
  0%|          | 0/12 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  8%|▊         | 1/12 [03:42<40:50, 222.79s/it]<startofstring> PCSK9 is a gene <bot>: and a gene-related gene gene gene
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 17%|█▋        | 2/12 [07:25<37:04, 222.48s/it]<startofstring> PCSK9 is a gene <bot>: which is associated with the expression of the
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 25%|██▌       | 3/12 [11:08<33:24, 222.70s/it]<startofstring> PCSK9 is a gene <bot>: which is associated with the occurrence of various
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 33%|███▎      | 4/12 [14:51<29:42, 222.86s/it]<startofstring> PCSK9 is a gene <bot>: and the prognostic value of the prog
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 42%|████▏     | 5/12 [18:33<25:59, 222.84s/it]<startofstring> PCSK9 is a gene <bot>: and is associated with the occurrence of a
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 50%|█████     | 6/12 [22:17<22:18, 223.07s/it]<startofstring> PCSK9 is a gene <bot>: and it is involved in the regulation of
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 58%|█████▊    | 7/12 [26:00<18:35, 223.01s/it]<startofstring> PCSK9 is a gene <bot>: and prognostic value is validated in the
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
 67%|██████▋   | 8/12 [29:43<14:51, 222.98s/it]<startofstring> PCSK9 is a gene <bot>: and prognosticnostic prediction is a new
 67%|██████▋   | 8/12 [31:32<15:46, 236.53s/it]
Split the dataset: Finally, you will need to split the dataset into training, validation, and testing sets. The training set will be used to train the model, the validation set will be used to tune hyperparameters, and the testing set will be used to evaluate the final performance of the model.

By following these steps, you can create a training dataset that can be used to fine-tune a transformer question-answer model. Once you have a suitable dataset, you can use it to train your model using techniques such as transfer learning or fine-tuning.




Marc Duby
can I then ask free form questions, or do I need to provide a context for each question
In general, it is better to provide context for each question when using a transformer question-answer model. This is because transformer models are trained to perform question-answering on specific contexts, such as a particular article or paragraph. Without context, the model may not be able to accurately understand the question and provide a relevant answer.

However, some transformer models are designed to work without explicit context. These models, such as GPT-3, are trained on a diverse set of language tasks and can generate high-quality responses to a wide variety of questions without explicit context.

So, whether you need to provide context for each question or not depends on the specific transformer model you are using and the task you are trying to perform. If you are unsure, it is a good idea to provide context to ensure the best possible performance.




Marc Duby
what type of question answering open source transformer model requires no context
One example of a transformer-based open source question-answering model that requires no explicit context is OpenAI's GPT-3 (Generative Pre-trained Transformer 3) model.

GPT-3 is a large-scale transformer model trained on a diverse range of natural language processing tasks, including question-answering. It is designed to be a "plug-and-play" language model, meaning that it can be used for a wide range of natural language tasks, including question-answering, without requiring explicit context.

When given a question, GPT-3 generates a response based on its understanding of natural language and its training on a large corpus of text. GPT-3 is able to generate high-quality responses to a wide variety of questions, even if no explicit context is provided.

That being said, it's important to note that GPT-3 may not always provide accurate or relevant answers to all types of questions. It is still important to carefully evaluate the quality of the answers generated by GPT-3, especially when using it for critical applications such as medical diagnosis or legal advice.




Marc Duby
do any gpt-2 models provide question answer capability without context
No, the standard GPT-2 models do not provide question-answer capability without context. The GPT-2 model is a language model that generates text based on the input context, but it is not specifically designed for question-answering.

However, there have been some research efforts to modify the GPT-2 model for question-answering tasks. For example, the GPT-2 model can be fine-tuned on question-answering datasets such as SQuAD or TriviaQA, which allows it to generate answers to specific questions. But, in these cases, the model still requires a context that includes the passage or document from which the question is being asked.

In summary, while the GPT-2 model can be fine-tuned for question-answering tasks, it is not designed for this purpose and cannot provide accurate answers without an input context. For question-answering tasks without explicit context, models like GPT-3 or other specialized question-answering models might be more suitable.




Marc Duby
does databricks dolly model need context to answer questions
Yes, Databricks Dolly model, like most question-answering models, typically requires context to provide accurate answers to questions. Dolly is a transformer-based language model that has been fine-tuned on a variety of natural language tasks, including question-answering.

When given a question, Dolly uses its understanding of natural language and its training on large amounts of text data to generate a response. However, the quality and accuracy of Dolly's answers depend on the context provided. Without context, Dolly may not be able to accurately understand the question and provide a relevant answer.

That being said, it's important to note that the quality and accuracy of Dolly's answers also depend on the quality of the training data and the specific fine-tuning process used. So, while Dolly may require context to provide accurate answers, the overall performance of the model can be improved by using high-quality training data and appropriate fine-tuning techniques.
