{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import col, struct, explode, when, lit, array_max, array, split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the variant input directory is: /Users/mduby/Data/Broad/Magma/Common/part*\nthe output directory is: /Users/mduby/Data/Broad/Magma/Out/Step1/\n"
    }
   ],
   "source": [
    "# vep_srcdir = 's3://dig-analysis-data/out/varianteffect/effects/part-*'\n",
    "# freq_srcdir = 's3://dig-analysis-data/out/frequencyanalysis/'\n",
    "# outdir = 's3://dig-bio-index/burden/vepbinning'\n",
    "\n",
    "# development localhost directories\n",
    "variant_srcdir = '/Users/mduby/Data/Broad/Magma/Common/part*'\n",
    "out_dir = '/Users/mduby/Data/Broad/Magma/Out/Step1/'\n",
    "\n",
    "# print\n",
    "print(\"the variant input directory is: {}\".format(variant_srcdir))\n",
    "print(\"the output directory is: {}\".format(out_srcdir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "got Spark session of type <class 'pyspark.sql.session.SparkSession'>\n"
    }
   ],
   "source": [
    "# open spark session\n",
    "spark = SparkSession.builder.appName('bioindex').getOrCreate()\n",
    "\n",
    "print(\"got Spark session of type {}\".format(type(spark)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the schema for the common variant file\n",
    "variant_schema = StructType(\n",
    "    [\n",
    "        StructField('varId', StringType(), nullable=False),\n",
    "        StructField('dbSNP', StringType(), nullable=False),\n",
    "        StructField('consequence', StringType(), nullable=False),\n",
    "        StructField('gene', StringType(), nullable=False),\n",
    "        StructField('transcript', StringType(), nullable=False),\n",
    "        StructField('impact', StringType(), nullable=False),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the loaded variant data frame has 1465865 rows\n+---------------+------------+\n|          varId|       dbSNP|\n+---------------+------------+\n|1:66159416:T:TC|        null|\n| 1:66160075:T:A|   rs6664534|\n| 1:66167207:G:A| rs544422243|\n| 1:66181653:C:T|rs1045050806|\n| 1:66184360:T:C| rs569568659|\n| 1:66189332:A:G| rs186428007|\n| 1:66203588:A:T| rs374304719|\n| 1:66210208:C:T| rs560512594|\n|  1:6621737:C:T| rs772700121|\n| 1:66247464:G:A| rs192224545|\n| 1:66257069:A:G| rs988459270|\n| 1:66281675:G:T|  rs72931395|\n| 1:66284502:G:A| rs567848519|\n| 1:66289594:A:T|  rs12021998|\n| 1:66291395:A:G| rs566719052|\n|  1:6631086:T:A|        null|\n| 1:66312242:A:G| rs113234023|\n| 1:66324905:T:A| rs543414819|\n| 1:66330519:C:T| rs565829829|\n|  1:6633230:G:A|  rs12132145|\n+---------------+------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "# method to load the frequencies\n",
    "df_load = spark.read \\\n",
    "        .csv(variant_srcdir, sep='\\t', header=True, schema=variant_schema) \\\n",
    "        .select('varId', 'dbSNP')# method to load the frequencies\n",
    "\n",
    "# print\n",
    "print(\"the loaded variant data frame has {} rows\".format(df_load.count()))\n",
    "df_load.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the non null RS id dataframe has 1295699 rows\n"
    }
   ],
   "source": [
    "# keep only the rows with non null dbSNP ids\n",
    "df_nonnull_load = df_load.filter(col(\"dbSNP\").isNotNull())\n",
    "\n",
    "# print\n",
    "print(\"the non null RS id dataframe has {} rows\".format(df_nonnull_load.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------+------------+\n|         varId|       dbSNP|\n+--------------+------------+\n|1:66160075:T:A|   rs6664534|\n|1:66167207:G:A| rs544422243|\n|1:66181653:C:T|rs1045050806|\n|1:66184360:T:C| rs569568659|\n|1:66189332:A:G| rs186428007|\n|1:66203588:A:T| rs374304719|\n|1:66210208:C:T| rs560512594|\n| 1:6621737:C:T| rs772700121|\n|1:66247464:G:A| rs192224545|\n|1:66257069:A:G| rs988459270|\n|1:66281675:G:T|  rs72931395|\n|1:66284502:G:A| rs567848519|\n|1:66289594:A:T|  rs12021998|\n|1:66291395:A:G| rs566719052|\n|1:66312242:A:G| rs113234023|\n|1:66324905:T:A| rs543414819|\n|1:66330519:C:T| rs565829829|\n| 1:6633230:G:A|  rs12132145|\n|1:66333368:A:G| rs753963099|\n|1:66348745:T:G| rs927568542|\n+--------------+------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "df_nonnull_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'shaow'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4b66b00ff548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_nonnull_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_nonnull_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'position'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_nonnull_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshaow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Energy/VirtualPyEnvs/tf2_37/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1304\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1305\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'shaow'"
     ]
    }
   ],
   "source": [
    "# decompose first field and get chrom/pos\n",
    "split_col = split(df_nonnull_load['varId'], ':')\n",
    "\n",
    "# add the first two columns back in\n",
    "df_nonnull_load = df_nonnull_load.withColumn('chromosome', split_col.getItem(0))\n",
    "df_nonnull_load = df_nonnull_load.withColumn('position', split_col.getItem(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------+------------+----------+--------+\n|         varId|       dbSNP|chromosome|position|\n+--------------+------------+----------+--------+\n|1:66160075:T:A|   rs6664534|         1|66160075|\n|1:66167207:G:A| rs544422243|         1|66167207|\n|1:66181653:C:T|rs1045050806|         1|66181653|\n|1:66184360:T:C| rs569568659|         1|66184360|\n|1:66189332:A:G| rs186428007|         1|66189332|\n|1:66203588:A:T| rs374304719|         1|66203588|\n|1:66210208:C:T| rs560512594|         1|66210208|\n| 1:6621737:C:T| rs772700121|         1| 6621737|\n|1:66247464:G:A| rs192224545|         1|66247464|\n|1:66257069:A:G| rs988459270|         1|66257069|\n|1:66281675:G:T|  rs72931395|         1|66281675|\n|1:66284502:G:A| rs567848519|         1|66284502|\n|1:66289594:A:T|  rs12021998|         1|66289594|\n|1:66291395:A:G| rs566719052|         1|66291395|\n|1:66312242:A:G| rs113234023|         1|66312242|\n|1:66324905:T:A| rs543414819|         1|66324905|\n|1:66330519:C:T| rs565829829|         1|66330519|\n| 1:6633230:G:A|  rs12132145|         1| 6633230|\n|1:66333368:A:G| rs753963099|         1|66333368|\n|1:66348745:T:G| rs927568542|         1|66348745|\n+--------------+------------+----------+--------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "df_nonnull_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out data frame and save magma variant input file\n",
    "df_export = df_nonnull_load.select(\"dbSnp\", 'chromosome', 'position')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------+----------+--------+\n|       dbSnp|chromosome|position|\n+------------+----------+--------+\n|   rs6664534|         1|66160075|\n| rs544422243|         1|66167207|\n|rs1045050806|         1|66181653|\n| rs569568659|         1|66184360|\n| rs186428007|         1|66189332|\n| rs374304719|         1|66203588|\n| rs560512594|         1|66210208|\n| rs772700121|         1| 6621737|\n| rs192224545|         1|66247464|\n| rs988459270|         1|66257069|\n|  rs72931395|         1|66281675|\n| rs567848519|         1|66284502|\n|  rs12021998|         1|66289594|\n| rs566719052|         1|66291395|\n| rs113234023|         1|66312242|\n| rs543414819|         1|66324905|\n| rs565829829|         1|66330519|\n|  rs12132145|         1| 6633230|\n| rs753963099|         1|66333368|\n| rs927568542|         1|66348745|\n+------------+----------+--------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "df_export.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the tab delimited file\n",
    "df_export.write.mode('overwrite').option(\"delimiter\", \"\\t\").csv(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce into a single file\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bittf237venv9b274482c7ba4966ad2cf02baa9bb24c",
   "display_name": "Python 3.7.6 64-bit ('tf2_37': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}